# GUARDNET ğŸ§ 

[![Python](https://img.shields.io/badge/Python-3.7%2B-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.12%2B-red.svg)](https://pytorch.org/)
[![CUDA](https://img.shields.io/badge/CUDA-11.6%2B-green.svg)](https://developer.nvidia.com/cuda-toolkit)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

> **From Global Search to Local Lookup: Scalable Knowledge Base Completion via Differentiable Guarded Logic**  
> *Under review at vldb 2026*

This repository provides the official PyTorch implementation for **GUARDNET**. The code is designed to be a complete and faithful reproduction of the paper's methodology, enabling researchers to explore and build upon our work in scalable neuro-symbolic reasoning.

---

## ğŸ“‹ Overview

**GUARDNET** is the first framework to leverage the **Guarded Fragment (GF)** of first-order logic as a principled inductive bias for robust and scalable neighborhood-based reasoning. It directly addresses the critical challenge of systematic generalization in deep learning, particularly in tasks requiring multi-hop inference, where existing models often fail.

### ğŸ¯ Key Features

- **Guarded Logic as Inductive Bias**: Employs the syntactic 'guard' of GF to restrict logical quantification to local, relational neighborhoods
- **Principled Fuzzy Semantics**: Built on differentiable fuzzy logic using **Product t-norms** and **Sigmoidal Reichenbach implication**
- **Hybrid Domain Training**: Novel training strategy combining **Core Domain** with **Latent Domain** for logical fidelity and robust generalization
- **Systematic Generalization**: Exceptional performance on challenging zero-shot, multi-hop reasoning tasks

---

## ğŸ› ï¸ Installation & Setup

### Prerequisites

The framework is built entirely in Python. All experiments were conducted on servers equipped with NVIDIA GPUs.

| Component | Version | Purpose |
|:----------|:--------|:--------|
| **Hardware** | NVIDIA RTX 4090 (24GB) | Recommended for GPU acceleration |
| **Python** | 3.7.0+ | Core framework language |
| **PyTorch** | 1.12.1+ | Deep learning backend |
| **CUDA** | 11.6+ | For GPU support |

### Dependencies
```bash
# Clone the repository
git clone https://github.com/your-username/guardnet.git
cd guardnet

# Install Python dependencies
pip install -r requirements.txt
```

<details>
<summary>ğŸ“¦ Full Dependency List</summary>

```
torch>=1.12.1
numpy>=1.21.0
pandas>=1.1.3
matplotlib>=3.3.2
scikit-learn>=0.24.0
tqdm>=4.62.0
loguru>=0.6.0
pyparsing>=3.0.6
```

</details>

---

## ğŸ”§ Data Preparation

The framework expects standard knowledge base completion datasets, split into training, validation, and test sets. 

The datasets referenced in the paper include:

- SNOMED CT
- Gene Ontology (GO)
- Yeast PPI
- Human PPI

### ğŸ“ Required Data Structure

Please place your datasets inside a `data/` directory, following this structure for each dataset:
```
data/your_dataset_name/
â”œâ”€â”€ entities.txt      # List of all entity names, one per line
â”œâ”€â”€ relations.txt     # List of all relation (predicate) names
â”œâ”€â”€ train.txt         # Training facts (head_entity relation tail_entity)
â”œâ”€â”€ valid.txt         # Validation facts
â””â”€â”€ test.txt          # Test facts
```

The axioms (rules) for training should be provided in a separate file or defined within the data loading logic, following the syntax described in the paper.

---

## ğŸš€ Training & Evaluation

The code has been structured into separate files for clarity. The main entry point is `main.py`.

### ğŸ“Š Knowledge Base Completion (KBC)

This is the primary task for evaluating standard performance on concept subsumption and link prediction.

#### 1. Training

Use `main.py` to launch training. The script will load the configuration from `config.py`, initialize the `GuardNet` model, and run the training loop defined in `train.py`. 

The best model checkpoint is saved based on validation performance.
```bash
# Example: Training GUARDNET on a sample dataset
python main.py
```

<details>
<summary>ğŸ”§ Advanced Training Configuration</summary>

The `config.py` file centralizes all hyperparameters from the paper's appendix. You can modify it directly to tune the model.

Key parameters include:

- `embedding_dim`: Dimensionality of embeddings
- `learning_rate`: Initial learning rate for AdamW
- `batch_size`: Number of formulas per training batch
- `weight_decay`: L2 regularization coefficient for AdamW
- `lambda_start` & `lambda_end`: Curriculum parameters for the hybrid loss
- `lse_temperature`: The temperature Ï„ for fuzzy quantifier approximation

</details>

#### 2. Evaluation

The evaluation logic is integrated into the training loop in `train.py`, which reports metrics after each epoch.

For standalone evaluation, you can load a saved checkpoint and run it on the test set:
```bash
python evaluate.py \
    --model_path checkpoints/best_model.pt \
    --data_path data/your_dataset_name
```

The evaluation follows the filtered ranking protocol described in the paper.

---

## âš™ï¸ Hyperparameter Configuration

The optimal hyperparameters found in the paper are set as defaults in the `config.py` file.

| Parameter | Selected Value | Description |
|:----------|:--------------|:------------|
| **Optimizer** | AdamW | Decoupled weight decay optimizer |
| **Learning Rate** | 5e-4 | Initial learning rate |
| **Embedding Dim** | 200 | Dimensionality of constant embeddings |
| **Batch Size** | 512 | Samples per iteration |
| **Weight Decay** | 5e-5 | L2 regularization coefficient |
| **Predicate MLP** | 2 hidden layers, 256 units, ReLU | Architecture for grounding predicates |
| **LSE Temperature** | 0.1 | Controls sharpness of quantifier approximation |
| **Loss Curriculum** | 0.9 to 0.4 | Linearly anneals from fidelity to generalization |
| **Early Stopping** | 15 epochs patience | Prevents overfitting |

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

<div align="center">
  <sub>Built with â¤ï¸ for the Neuro-Symbolic AI Community</sub>
</div>
